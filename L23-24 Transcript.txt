SPEAKER 0It It's yeah. Yeah. Yeah, that's good. I I Uh Yeah Oh. No I. I. Like. Where's the one with fucking? Hi everyone.SPEAKER 1Oh wait, it's not showing the screen yet. Why isn't it showing the screen? There we go. Whoops.SPEAKER 0Hello.SPEAKER 1So, we didn't quite get through the whole, the, the last bit of the last lecture, but I'm gonna put it in this lecture in the, in an appropriate place. So we, we'll still start off with this one. And if we run over this time, it's all right cause the last lecture is a bit short, so there's some extra time if there's some spillover, we'll still get to everything because the last half of the last lectures. Kind of a mini lecture. All right. So also, I wanna let you know that the final exam info is all on the canvas now, where it has sort of information about the structure and studying tips and things like that. Also, your last tutorial next week is going to involve, is preparing for the final exam. Um, and so you'll, you'll sort of learn about that then. Uh, and we're also, Caroline and I will each have a sort of Zoom Q&A like a week or so before the exam. So we can, and we can record it. So if you're not available at that time, you can, you know, add questions to the discussion board, and then we can read off your questions and answer them on during the meeting, and then you can watch the recording later. Uh, that will be announced on canvas as well, in case you're not listening. All right. So, or forget, whatever, either way. Um. Right. And, and you guys are working on the papers. That's that, right? All right, yeah. Uh. All right, so now we're talking about, and today we're gonna have. One more kind of back and forth between nativists and constructivists but here about language. Now We're gonna talk about is Well, the, the concept of universal grammar, which is this concept that sort of famously associated with Noam Chomsky. And we're basically looking at the complexity of syntax to just talk about uh why language must be innate. Um, we're gonna look at evidence support from child language productions, and we're also gonna look at. Looking at different types of nativist arguments, different types of evidence, such as some sensitive periods in children generating language, in Nicaraguan sign language. And here's what I'll bring in the, that bits from last time, uh, and where we're all, all sort of looking at what happens, what, with, when deaf children are of hearing parents get together and create a language. Um. And then we're gonna look at novel, we're gonna look at how a simpler form of nativism that looks at uh evidence through verb learning. We'll, I'll explain all of that. All right, now, let's talk about, now these are some examples that should be reminiscent of the examples from the first lecture where, when Chomsky was sort of criticizing um Page's explanation for language development. So, what syntacticians. We are talking about. We're gonna talk about what are syntacticians up to, right? So Chomsky was a syntactician where he sort of wrote out the kind of representations that underlie human syntax. And understanding these representations, the kinds of things he was doing is critical for understanding why he thought learning couldn't get you to these representations. All right, so here's, here's some example of language knowledge it says we all have. All right, now this is an example of when you ask a question. All right. Let's say you take a statement and you turn it into a question. Right now, often what you do, sometimes you take the, you, you took the question where like the WH word like who or which or whatever, often those go to the front of the sentence. OK. But grammatically, there was like a place that that like could have gone, right? So, so here's a question, which movie does Susan imagine that Sarah saw last night? Right, you could say Susan imagined that Sarah saw which movie last night? See that little like T, that little T indicates that's the trace, and that sort of indicates where this witch movie would have been in like a kind of regular sense. So imagine you knew the name of the movie, you could have said, Susan, imagine that Sarah saw Jurassic Park last night. Right? The sort of noun Jurassic Park, which would be the answer to the question which movie goes where that little T goes in the sentence. Alright, so John was wondering who Sarah decided she would go to the movies with on Sunday. So you could say, you know, Sarah decided she'd go to the movies with Bill on Sunday, right? So that little tea is where Bill goes, but because John doesn't know who it is, the who comes over here. I didn't like the movie which you said that everyone was talking about the other day. Right, same thing, you may, you may take whichever sandwich you find on the table over there, right? You may take, you know, it's like you, you know, you may take the ham sandwich you find on the table over there or whatever, right? That movie, I wouldn't recommend that anyone consider taking their kids to. Right, so these are all kind of moving this, you know, referring phrase around, and these little T's are kind of like where it would go if you kind of structured it like a regular sentence. All right, and those are all grammatical sentences in English. That movie, I wouldn't recommend that because they're taking their kids to. Now here are all a bunch of examples that are not grammatical. What did Beth eat peanut butter and for dinner? Right? That is not a sentence, right? You would, but why not? You, you know, you could say, Beth ate peanut butter and what for dinner? So here the what can go in the place of the T, where the T is, but you can't move it to the front of the sentence, unlike with which movie. In that first example, who does Sam know a girl who was in love with? Who does Betty know which professor flunked? What food were you never aware of the hypothesis that you shouldn't eat? Right? None of those, these are all sort of illegal moves according to our syntax. But how would you describe the rules that let all of these sentences be allowed, but none of these sentences? Right? So it's not like you can't, so clearly there's all kinds of situations where you're allowed to take the, the, the, the, the which movie, the what, the who from some part of the sentence and move it over. There's a whole bunch of situations where you're not allowed. Right? And so what a syntactician would do, or someone like Noam Chomsky would do when he wasn't, you know, protesting uh the Vietnam War or something like that back then, he would be sort of writing out the sort of representations of grammar that would generate, if you were gonna like say code a computer to say like generate sentences following these rules, you would have to sort of write a computer code, you have to write the rules such that these sentences were allowed in the, at an 11. But it would never come up with 12. Like if you're writing out the rules, and it came up with 12, you knew your rules were wrong because no human, because English doesn't allow for that. So, and this is the kind of thing that linguists are like. What did you, how did you learn this? You've never heard sentences like 12 before. No one sat you down and said, don't say it this way, cause no one has ever said anything like that. Right, those just aren't in the input. All these ungrammatical sentences were invented by linguists to illustrate how syntax works. These don't occur naturally. But how would you take all, but if you were just learning from the input. You would just think, oh, I can just move what I can take any noun and just sort of replace it with what and and move it to the beginning of the sentence. In theory, you could do that, right? But the point is that the point is these people, you can't do that, and your genes are telling you why. Your genes are telling you what kind of moves are allowed in syntax and what kind of moves are not allowed in syntax. Now you have to learn the particulars of your language, but they argue that some of these rules are universal across languages. Right? On the surface, languages have different word orders, they have different vocabulary, different sounds, but there's some deep structures of how the syntax works around how you can move words around. Some of these things are allowed in in languages, and some of them are never allowed in languages. And that's what they're saying is what your innate endowment is, or that's where they get evidence of your innate endowment. And here's a video with Steve Pinker saying something similar. Casey didn't believe me.SPEAKER 2There are a number of misconceptions about human language. One of them is that parents teach children to speak. We know that that's not true because children who grow up without speaking parents but with other children can develop a sign language just by spontaneously interacting. We also know that even when children do have parents, they pay much more attention to their peers. If someone moves from one country to another, they may never master the new language, but their children will pick it up much better than they do just from interacting with other kids on the playground. Another misconception is that rules of grammar are things that you learn in school or from editors or English teachers. In fact, there are far more sophisticated rules of grammar that just determine the order of words in our ordinary conversation, much more sophisticated than what your school teacher ever taught you. Universal grammar is a term invented by Noam Chomsky to refer not so much to a list of the things that all languages have in common, but rather to the Underlying logic or plan or guidelines for human language which he proposes is wired into the brain and allows language acquisition to happen so it's not that anyone is born with language, obviously we're not born with English or Japanese and so on. We know that because of immigration. If someone moves to a new country, their child will not speak the language they speak, but the language of the environment. However, The question is what makes language acquisition possible to begin with. Why does a human learn to speak but a cat won't, even if the cat hears the same sentences? How come the best computer program in existence can't learn to speak the way a child does? Well, the child obviously knows precisely how to analyze the speech coming from other parents, how to chop it into words, how to peel off. Rules that order nouns and verbs and subjects and objects, and it's that ability, the ability to hear a stream of sound as being composed of grammatical elements that you can call universal grammar. The hypothesis would be that there are differences in the precise genes between humans and other animals just like there are differences in genes between any other two animals you want to compare. And that those genes change the way the brain develops in utero and after birth that gives the brain the ability to wire itself up in a way that allows the brain to analyze speech or gestures in grammatical terms. So it would be a certain tendency for the brain to develop circuits for analyzing signals from other people as having a grammatical structure that can be used to convey meaning.SPEAKER 1All right. All right, so what's this argument look like? So when Noam Chomsky was first coming up with this idea for this being innate, He also was sort of arguing that this might be language-specific, but was innate. This is how he was first doing because he was looking at psychology. So what's learning look like? Let's look what talk to the psychologists, right? So there's something like what BF Skinner and the behaviorists were talking about, like some sort of associative learning, learning from reinforcement. Then there was a sensory motor learning exploration that Page was talking about. He looked at that and said, that cannot explain human syntax. You don't, you can't get to human syntax from these mechanisms. And he even sort of showed, for example, reinforcement learning, he had a sort of computational description of what that mechanism looked like and sort of showed that the complexity of language syntax is beyond the computational uh like uh uh capabilities of a reinforcement learning system. Um, So, there is a human language is too computationally complex. Language is composed of abstracts and tactic rules that can generate an infinite number of novel sentences that don't even have to make any sense. So we had this example called colorless green ideas sleep furiously. Now, that is a sentence that means nothing, but you can recognize it as a grammatical English sentence, right? That's, you know, there is a, uh, you know, adjective, noun. Uh, you know, so, so color the screen ideas, right, those two adjectives that are modifying the noun ideas. The verb is sleep, and the adverb furiously is modifying sleep. So it's a totally grammatical English sentence, it just doesn't make any sense. But that was an example of a sentence you could just say, you can come up with, right, from not having this abstract knowledge. Now, a key thing that Chomsky talks about is recursion. Now, this is a sort of a structure of symbolic systems where You can embed something within itself, but the same type of thing can be embedded in itself. So for example, sentences can be embedded within sentences. What do I mean by that? So if you have a sentence like the man is tall, you can have within that you go, the man who likes cats is tall. So what's the sentences, you know, a subject, the man is tall is the predicate. The man who likes cats is tall, the man who likes cats went to the store yesterday is tall. The man who likes cats went to the store yesterday to pick up a new tie to match his shirt is tall. Right, you can just keep on doing that forever. You can just keep on stuffing more and more stuff, embedding more and more stuff within a sentence, and the symbolic system sort of lets you do that. Now, eventually, the sentence would get so long that you couldn't keep track of who was who anymore because you have sort of these memory limitations. But Charms would argue that's the fault of our memory. Our syntax actually allows you to do this, uh, forever, basically. There's no rule of syntax that says you can't keep on stuffing more sentences inside of a sentence. And so he argues that this type of recursion, you just couldn't possibly get that from some sort of associative learning mechanism or something that Page's general developmental mechanisms was talking about. So he was making these arguments, so he wrote this, so he, so he sort of famously wrote this review, so. BF Skinner wrote a book called Verbal Behavior, where the goal was to explain child developing language from reinforcement. Applying basic like dog training types of meth, you know, techniques, right, to to explaining how children learn to speak, and Chomsky sort of wrote a review, this sort of, there's this famous review he wrote of the book where he's basically like this is all hogwash. Um, which sort of, you know, which, which made his argument, which made his position famous. He was a linguist, people in psychology didn't really know anything about linguistics, but this sort of like helped bridge these sort of disciplines and their treatment of, of the, of these ideas. Right, and then there's, there was an argument that if you then, and then he sort of was claiming this claim has gotten modified over the years a bit, but if you compare, if at the time you were comparing it to all the kinds of animal communication systems, you couldn't sort of analyze the structure of their communication and show anything like recursion. Now, people have analyzed whale communi whale and dolphin communication, which appears to show a recursion in its patterns. Um, but that would be have been independent, that would have been. Independently evolved, right? No other primates show recursion in their communication. And so the fact, so it's sort of interesting that another develop another evolutionary sort of, Tree has rediscovered some of these same communicative um powers. But anyway, interesting stuff going on in the world of dolphin and whale research, but Uh, anyway, right, and also there's some evidence that some birdsong, but, but with birdsong. The meaning isn't tied to it. So it's not just that we just have recursion, it's also that we, when we move our words around, it changes the meaning. When the birds, if you analyze just the sort of acoustic structure and repeating patterns of birdsong and to kind of show there's some depth in how the patterns work, there's no reason to think that if you kind of change it around a little bit, it would like be a different song, right? They're not communicating specific messages to each other in the same way. There's no evidence of that. Right, just kind of like, this is my tree, let's mate, stuff like that. All right, so Uh All right, so All right, so where does that knowledge of language come from, right? So at some level, clearly our genetic endowment has the ability to learn language, all its complexities, right? If we couldn't learn. If we, if our genes did not support our language learning, then we would not learn language. There's nothing we can do that our genes haven't like licensed us to do. Right. But, OK, but what specifically then is in, is sort of universal grammar about? Now, one of the key things is that it's about having structured representations with abstracts and tactic categories. That's one sort of key element. For example, sentences are composed of noun phrases and verb phrases. I'm about to define all these terms shortly, and verb phrases are composed of verbs and noun phrases. Rules, and here's the key thing, the rules of syntax operate over phrases. Now what is a phrase? You might be wondering. Now, a phrase is a group of words that form a functional and structural unit. For example, a noun phrase consists of a determiner, so the or a are called determiners. You're not gonna be quizzed on all these particular linguistics terms. You should know what what phrase is, but things like determiner, I, you know, it's not gonna be an exam question that's like, what's a determiner? All right, so a noun phrase consists of a determiner, a noun, like the dog. One can also add modifies such as the dog by the house. Buy the house is like which, which dog, the dog by the house. That's all a noun phrase, cause it's still all about the dog. Right? These modifiers have phrases with them like buy the house is a prepositional phrase, buy is the preposition. It has another noun phrase than that, the house within it. Phrases are hierarchically organized. So, let's look at a nice little syntax tree here. Now, the S, this is some linguistics notation, the S represents a sentence, that means a sentence, and the sentence is broken down into a noun phrase, the house, and a verb phrase is red. OK. So the verb is And the adjective red part of the verb phrase. Now, let's look at how this looks for a sentence like the house at the end of the street is red. OK, so now see how like these things are embedded within that. The embedding, what I mean by hiracle. So. So the house at the end of the street is a noun phrase. D is determinate, is, is the determiner, and house at the end of the street is a noun phrase within that noun phrase. This noun phrase is composed of a noun, house, and a prepositional phrase at the end of the street. That prepositional phrase is composed of a preposition at, and a noun phrase, the end of the street. That noun phrase is composed of the determine of the and the noun phrase end of the street, which is composed of the, the noun end and the prepositional phrase of the street, which is composed of the preposition of and the noun phrase, the street, which is composed of the determine of the and the noun street. All right, so look how many noun phrases and other phrases are all embedded within these phrases. Right? And this is a sentence, the, the house at the end of the street is a sentence that like a 3 year old would could understand, right? This isn't like a complicated sentence, but there's all got all this stuff in there, all right. So this is all kind of, all this complex structure is embedded in there. Now, how can you tell? This, this is a bunch of people drawing diagrams to make it complicated, but it's really not this complicated. So how could you tell what is in a phrase? One way to do it is by looking at reference in a pronoun, right? Like it is red. It can take, so if you were to say the housing in the street is red, it Right, takes an entire noun phrase, the house at the end of the street. You don't say it at the end of the street is red. Right, you kinda can say that, but it's weird. Or you wouldn't say the it is red. That's also weird, right? It takes the place, so all of this stuff, you can tell that all of this stuff is embedded in one noun phrase because it takes the the place of all of it. Right? You could say a man with dark glasses is following us. You would say he is following us, not he with dark glasses is following us, because he is, you know, is taking the place again. The entire phrase We watched a documentary of making cheese about cheese making last week. We did that last week. We didn't say we did that about cheese, so did that is kind of like it, but for verbs. Right? So we did that about cheese last week. It does, we wouldn't say that, right? So there's something about documentary about cheese making that all kind of combines into a phrase, cause they did that, or so. Takes its whole place. I, I'm just sort of, not that you have to memorize these tests, I'm just sort of giving you an example of like, When they're, when, when syntacticians are developing these representations, they have to sort of, they have to be able to predict what's allowed in the language and what's not allowed in the language. And this is one sort of way to say, well, we know we have this type, we have evidence for this type of syntax by what, how pronouns work. All right. Now, so people are basically saying, so part of what they're saying about Universal Grammar is that all this structural stuff. Is all kind of innate, not that any particular structure is innate, but that we that we create these structured representations is innate and potentially a couple of constraints on the, on the structures as well. So, so here's this quote, People obtain knowledge of the structure of the language from which no evidence is available in the data to which they are exposed as children. What is innate and what is learned. For example, they could say, well, you're all, all languages have something like verbs and nouns. So they all have words for actions, they all have words for objects. So maybe that's also kind of in there, that we have to structure our and all sentences have kind of like a, you know, a predicate or verb that just is, right, some kind of all sentences all around the world have a noun and a verb in them. So this seems to be like part of our universal endowment of how we create language. But then, the languages vary in how they do this, right? So, you would say, I kicked the ball in English, but in Turkish, you might say I the ball kicked, right? So they're sort of saying, well, in your particular language, you have to learn how these sort of this, the idiosysyncrasies of how these things are expressed, but you're coming into the world ready to understand that there's like words for actions, words for objects, and you kind of put them together into sentences. Is one sort of version, there's a few different versions of these claims, that's like one strong version that we have all this information in our, in our genes. And then also, how you might learn is sort of. I syntax-specific. So it's is similar to how the core knowledge modules. Remember before we're talking about core knowledge, how there's a module for objects and causality, and we're talking about the causality one, but there's an objects for, there's, there is, you know, uh, these particular potential triggers, right, that trigger you to apply your innate knowledge, right? And just like certain co surfaces, you know, co-moving surfaces in the object unity from what we talked about 65 weeks ago, whenever that was, um, it's similar to here, but here, it's sort of like a sequential nature of speech or the sequential nature of signs might sort of trigger your, this is language, and once you kind of recognize it as language, you then might start applying your special language modular processing. Sort of figure out what kind of language is this? How does this language work? All right, so all of that so far was not empirical in the sense of none of this sort of talk, looked at kids' speech and went like, how are they saying these things? Let's do an experiment with some kids. All of this stuff is like syntactic analysis and sort of formal arguments about what type of machines could learn it versus not learn it. And then saying, well, humans must be the kind of machine that learn it. Alright. So now we're gonna get into one example of people, of someone testing this, these predictions. In an experiment with kids. So This is going back to question asking, which is one of the reasons why I introduced all those examples. So, if there is, if you say the boy is crazy, you might turn that into a. Sentence, I'm sorry, uh, question, as you might ask it at, is the boy crazy? So how do you turn a declarative sentence, the boy is crazy to is the boy crazy? Well, You know, you sort of took it, you sort of put an is at the start of the sentence. Now what would the rule be if you were trying to tell someone the rule for sentence formation? Now What if it's, OK, well, the boy is crazy. OK, well, the is is like some sentences have more than one verb, right? This one just has one. So what if the rule is something like, take the first verb you come across and move that to the start of the sentence. Now, Stephen Crane says structural dependence is an idea that syntactic rules. Care about the structure of sentences. They depend on the structure, and what does that mean? So All right, let's say you say to form a question. You just take the is and go to the front. All right, what about if you say the the boy who is smoking is crazy? Which is do you move to the front? Do you use the first is? So is the boy who's smoking as crazy? No, you don't use the first is, right? So this first is rule doesn't work cause you say is the boy who is smoking crazy. So how would you know which is to move to the front? Well, the answer has to do with phrase structure. So the boy is, the boy is crazy, and the boy who is smoking is crazy, that is crazy is. The Is the verb phrase closest to the S in the syntax tree. They have the same structural position. So look at that, look at one more of these, right? So the boy is crazy, right? See the is here is part of this verb phrase, that's right below the sentence node. And the boy is over here. Here, the boy who is smoking, right? So who is smoking? Is within the noun phrase. So you don't take that is, you take this is. Right? So, when you're thinking about, so the rule for making sentences cares about. The structure, right? It says, oh, take the verb that's the closest to the sentence node, right? It's only down one layer. This is down more layers, right? It's within the noun phrase. You know, if you were to do this type of thing, right? It would be like all the way down, it'd be like down in here. All right. So What the Universal Grammar people then say is. Children will never ever violate universal grammar. So that means They will never say, is the boy who's smoking is crazy. Alright, this isn't just a T test where they're slightly less likely to say it. They're like, there will be 0 instances of this.SPEAKER 0That's the theory that's within that's actually how that.SPEAKER 1Well, what that's, we have to get to that second part. Right Council.SPEAKER 0Yeah, yeah.SPEAKER 1It looks like from Cran Nakayama in 1987, that this is true. They analyzed a corpus of child speech. There wasn't a single sentence of this form. Is the boy who's smoking is crazy. Now, of course, there you can imagine lots of other sentences that have that structure, not literally just those words. Right? But basically they analyzed what this type of error would be. What would, what would a structural dependence error be, where they kind of, where they made an error in the question formation that violated how phrase structure works. In 1987, Crane Nakayama said, he said we looked at naturalistic data, there's not one example of it. They also then did an elicited production study where they sort of showed a scene and tried to get kids to ask some questions about it, like, oh, look at the boy, he's kind of crazy, now ask me, they're like, oh, is the boy crazy? They sort of would like have these scenes, read it to them and get them to say stuff, ask them questions, and in this elicited production study, not a single kid made one of these errors. So in the naturalistic data, and in the study, 0. Now, it was 1987, was the sample that big? Probably not. I can't remember exactly, but back then, people didn't really have big samples. We didn't really like learn how that was bad yet. But still, You know, pretty good For now we're gonna say they're winning the argument. In the second half of the lecture, we're gonna go back and revisit this. But for now, all right, this is strong evidence for an innate schema. Of structural dependence. All right, so I know there's a lot of linguistics to get to, but you have to have all of that to understand why they would make this prediction. So, well, it's not that. They don't ever make errors of any kind, is they don't make this kind of error. So they're still, so, yeah, thank you, that's a good question. They're, they're at an age, I think they were like 6 or something, where they still make some ungrammatical things, but part of the theory is. They're going to make certain kinds of errors, but those are all going to be within what universal grammar kind of allows. And not making errors that are kind of outside the scope of how language works in the most abstract sense, right? Like all those, like all those types of examples like this, like there's other, um, not off the top of my head, I can't think of what one of these other errors would be, but there's all kinds of speech, people make speech errors all the time, right? We, that's certainly the case that if you analyze speech, it's full of people saying the wrong thing and then changing what they're saying. I do that constantly. I'm constantly interrupting myself and changing my sentence, right? So that happens a lot. Um, but they would sort of say, you're not doing this in a way that goes beyond the constraints of what our universal grammar says. All right, now we're gonna switch back to last week's. To get the context. All right, so that's one form of evidence, right? So, so, so far, the evidence for, so if you're putting together your case, right? Remember our learning outcomes are all about putting, making arguments for theories, right? So if you're putting together your, you're, you're making the case for, I'm gonna debate and I'm gonna support Chomsky's ideas. Here's all the evidence I'm gonna put together, right? Um, you, so far we have this kind of idea from the logic of the complexity of syntax. We have some predictions of child language. Another source of this comes from what's known as sensitive periods where. If children are not exposed to language in a by a certain age, they never learn language. Right, so now, how would you find such a child? Right? So, you only kind of find, so one area where, so there are a couple of famous examples, they're not great. They were kind of, people kind of stopped talking about them, but because of these ideas became really popular in the 70s, um they were really excited, for example, there was a feral child that people found that was like, You know, or there was this, there was this poor abused girl who was like trapped in her basement for like 6 years, you know, 7 years, and then they kind of, then she's adopted and like, let's study her language. But like there's so many other things happening if you are trapped in a basement your whole childhood, that like you're not really isolating the variable of like, not exposed to language, right? There's like a lot of other stuff going on, but We talked about them previously with home signers. If you are the deaf child of hearing parents, right? That's the closest thing you kind of get to a child who's sort of otherwise raised normally, but doesn't have language input, right? Of course, if your parents learn sign language, that's that's different, but that's sort of the situation where you're kind of like, what happens with deaf kids who are. Uh, who don't see sign language until different ages, and how does that affect them? Um, so we're gonna hop back over here. Yes, cause we didn't get to this at the end. Um, Oh yeah, maybe I'll do this in a couple different. Oh yeah, why don't I start? Uh, no, I'll do, let's, let's do it. Here. Why not?SPEAKER 3All right. Nicaragua, Central America. Managua, here, as in other places of the world, there are those who hardly have any language at all. Maria No Name, Mary, no name, deaf since birth, she has been isolated all her life, both from the people who could help her and from others with their disability. Her friend, linguist Judy Cagle understands the depth of her isolation. The two can communicate just a little using simple and primitive gestures.SPEAKER 4The first time I met her, she was missing the ability to tell me who she was. She was missing the ability to tell me how old she was. She doesn't know her name. In order to tell me who she was, she had to take me home and show me the papers and pictures of her family. Um, we had to share a context. She can tell me things. I can show you a bit. She can tell me what happened to her father. I asked her about her father dying and she said 3. OK, what 3 meant was she was shot 3 times. I know this from working with the other deaf signer that she said she was shot in 3 places, and that's how her father died, right? Yeah, right. And, uh, you know, but, but 3 is just not enough to give me access to the information that I would have needed had I not had prior knowledge about that. Ha. Oh Yeah. OK. What she's saying is, I, I had a daughter that went away and got married and that was it. She never came back. I had a son that went away. And I never heard from him again. You know, that's it. I'm alone. That's my life. She was language ready. Um, the problem was she didn't get access to language within that critical period, and that critical window for learning language in the way that we learned it is closed.SPEAKER 3This window for language remains open until we reach age 7. Then it slowly closes as we advance towards puberty. Before the 1980s, many deaf Nicaraguans were like Mary No Name. They never encountered the window for language because they never encountered others with their disability. But in 1980, after the Nicaraguan revolution, the new government tried to enhance deaf people's lives. It brought deaf village children into Managua to end their isolation. Here, educators try to teach them an existing sign language. The effort failed. The children showed little interest in learning a language forced upon them. Instead, they began communicating with each other in their own way. Judy Cagle was summoned from the United States to sort out the problem.SPEAKER 4I came down thinking wherever there were deaf people there was a sign language and that obviously there would be a, a full blown sign language in full swing here in Nicaragua and they, I said, well, you know, I, I can learn a bit of their sign language if that's what you want and, and work with you on learning it. They said, no, they don't have a sign language, they have, they have mimicas. They have mime gestures, and they pointed to a group of kids and said, we want to know what they're talking about.SPEAKER 3It turned out they were talking about a lot more than anyone dreamed possible. Cagle had arrived in Nicaragua shortly after the birth of a new language.SPEAKER 4Language needs company. Language needs a community. Language needs some sort of a trigger, and I think that, I think that trigger is, it's not so much that it needs a community in the sense that there have to be lots of people, but a commu part of being a community is wanting to share information with each other.SPEAKER 3Might this moment resemble what happened around 50,000 years ago? The turning point that led to the explosion of human creativity. Language does not need a voice. It is our legacy, an inevitability of being human. Today, we still don't know exactly when language evolved, when it opened the door to our phenomenal success as a species.SPEAKER 4This is a verb reduplicated, but language, every language, dependsSPEAKER 3on strict rules, all of them familiar.SPEAKER 4That's a role shift to looking at a man looking at the bird, then back to the man falling off the mountain, dreaming that he's going to fly like a. While many species can communicate, even vocalize, only human languagesSPEAKER 3are driven by complex rules. Every one of our world's 6300 languages has them. We call them syntax. In her isolation, Mary No Name never encountered syntax, but it is commonplace in the children's language.SPEAKER 4Syntax isn't the set of rules that you learned in your 3rd grade grammar that you had to memorize so you spoke English the way you're.SPEAKER 1Every linguist linguist will say that. I'm sorry. So, um. All right, so a few things. On that. Right. So, That was, so that, so that was the phenomena. And then people started trying to understand what is it, what's ha, what, what are kids doing specifically during this time. So, uh, during, so one place where people have shown Get, get some access into this kind of situation is a less extreme case than we just saw. And we're gonna go back and forth between a few different kinds of data while talking about this. But, right, so remember that that Mary no name has something pretty similar to, to, to home sign, which we talked about before when we're talking about the spatial cognition stuff, right? There's kind of this gestural communication system that does some basic things, but doesn't have syntax, doesn't have all this other stuff. Now, if you're not exposed to sign language at puberty, as they say, they most likely never get beyond home sign. However, there's also another form of this evidence about the sensitive period that Children can actually learn from parents and like be better at it than their parents. So let's say you have a case where you have parents who have, who are hearing, have a deaf child, and then they're like, well, we want to learn sign language so we can communicate with our child. Right, so this study looked at those cases and sort of found that the kids were making less errors than the parents were, even though their only input was the parents. Right, so the parent, so this was a case study. Right? So, now I remember those terms like inflectional and derivational morphemes like inflectional, you know, morphemes like um in, you know, jumping, jumped past tense, right? Or, you know, pluralizing, you know, destruction versus the story, the shun is a morpheme, right? So all these sort of ends of our, all these little bits, prefaces and suffixes that change the grammar, uh, like change the sort of grammatical use of the word, right, that inflect, you know, plural, past tense, this type of stuff. Right? Now, these are parents who are like, I want to learn American Sign Language to communicate with my deaf child. Lovely thing to do, right? And they sort of had, and they were analyzing these parents, these like small numbers of parents and kids, and the parents would kind of do, would only be correct about 70% of the time and turn into according to American Sign Language, like rules. However, the 7-year-old children who only at this point had only communicated with their parents, were being accurate, like over 80% of the time. Right? So they were actually doing. And which was the same as, you know, a sample of native signers at that age. Right. Also, to be clear, there are some, uh, there are some language delays when you are uh a deaf child of hearing parents who are learning sign language, but if you are a deaf child of deaf parents who are fluent in sign language, then there's no delays at all in terms of their infant, you know, language development and stuff like that. So it is that the that there's this noisy signal, and the children are like exploiting. The noisy signal. They're finding the signal, sorry, they're, they're finding the signal, the noise and exploiting it. They're kind of taking the little, the consistency that's in there and doing that and sort of filtering out the random stuff. So, Hudson came in Newport, then tried to show the same kind of pattern. With a large sample of hearing children learning sarcastic artificial grammar. All right, so, a bunch, so this is then basically, they created a fake mini language for an experiment. A bunch of experiments on artificial grammar learning is when you kind of create. A small little grammar, with a couple of kinds of sentences, a coup, a few different words, sometimes it's completely nonsense sounds and we're gonna talk about some of those, and sometimes they use real words. Um Well, actually, well, well, well, these. So, so the task is, like, They're sort of told that this is a kind of silly language that they have to learn. All right. And there were 17, there, there are 7 sessions of 20 minutes to learn this fake language. There were 17 words, there are 4 verbs, 12 nouns, and 1 determiner. And there are two conditions. One is the determiner always Came before the noun, and other times we only did 60% of the time. Participants were then were then given like to produce the language. And they always, in one way, such as the job the bear or object known as the fruit. Right, yeah, so, so they were like, do you always use the terminator or not? Now, What we're sort of seeing is Look at adults versus children. The children are more likely. In all cases Sorry, and these were, sorry, I should be clear that these were like, I'm just saying the bear falls in this case, but I think it was all, in this case, it was all sort of made up. I'm just making it more clear by using those real sentences here. Right? So in this real, sorry, in this uh uh fake language, look at how many kids are sort of very consistently using the determiner, right? And if you look at the percentages. The adults pretty much match the probability, right? In the 100% condition, they use the determinator 100% of the time. In the 60% condition, they're kind of using it 60% of the time. Right? But children are a bit more random in the 100% condition, they're only using it like 90% of the time, but they're also like using it much more than the adults in the 60% condition. There's way less of a difference between 100% and 60% for the kids. They're like in both cases. Trying to kind of use it as much as possible, well compared to the adults which have a much bigger separation, right? The adults are what's called probability matching. They're just sort of the adults are are just repeating back the input as they got it, while the children are kind of over-regularizing, over-regularizing where they are, even when it's stochastic, they are making it more relatively more deterministic. Right, this tries to simulate that kind, oh yeah.SPEAKER 0And also following the rules given to them, the kids are in.SPEAKER 1Or they are, well, yeah, they, they're kind of finding the rule, well, they're still finding the rule in this weird language, but then they're following, they're following the rule and following it all the time, where the adults are like, this is a rule, but it has exceptions, and so we'll do it sometimes, sometimes not. Yeah, yeah, yeah, so when it is, so when it, when it, when it is 100% of the rule, they, they do it 100%. But when it's 60%, they're like, OK, we're gonna do it 60% too. So it's like some, so imagine that 60% of sentences you had to say the, the before the noun, but in 40% of sentences, you could just say like bear, you could just say like bear fell asleep versus the bear fell asleep, right? So, If, if 60% of the time, if 40% of the time, you could just say bear fell asleep, right? You might just do that. But the kids are like being, even though you only said the bear 60% of the time, I'm still gonna say the like 80% of the time. So they're like boosting, they're like using the the rule more frequently than the rule was used in their. In their input, right? That's, and that's supposed to be analogous to what happens to those signers, right? Those, those, those deaf, those hearing parents who learn sign language kind of badly, right? We're sort of using the correct syntax a bunch of the time, but would mess it up a lot too. And the kids would kind of find the right syntax and use it more frequently than the parents themselves did. So the idea is that they're kind of like boot, they're kind of finding the rule and making it more regular than the input is even sort of telling them to do, like for directly. This is one of the examples of them sort of being constrained and how they're not just, they're not just absorbing exactly what they're getting and spitting it back. They have like an internal constraint to sort of make things more rule-like. And that's, and, and one of the ideas that this is a sort of universal grammar thing is to kind of make, it's like kids are like, these are syntactic rules. So we're gonna like make it rule-like. It's sort of the the idea, even though it's completely made up language, they're like applying this to this made up language. Is, is the, is the, is the, is the universal grammar interpretation. We'll give you another interpretation in the second half of today or next time depending. On how far we get. All right, so now we'll switch back to To this one. Now, all right, so that's what sort of. All right, so that was deaf kids of that was deaf kids of parents, right? So they, so, so the kids were over-regularizing, right? The the, the deaf kids of hearing parents who were kind of bad at sign language got better at sign language than their parents were, right? And then you had this kind of same behavior in this fake, fake language study, a little mini fake language, right? Now we're going back to Nicaraguan sign language. To look at how it changes over time, right? So Nicolonwan sign language, right? There was a bunch of kids who showed up, got together and sort of made up this language. Um, So This paper looked at the compositional structure of natural sign language. So we talked about so. One way that what we mean by compositional structure is that meaning is split up into components, so they can be recombined to generate more kinds of meanings. What do you mean by that? So think about the, the sentence, the ball rolled down the hill. Right, that's one motion. There's only one mode, if you're watching that event, the rolling and the down are not separate from each other, right? The rolling is the manner of motion, and the down is the direction of motion, but it's just one thing that's happening. But our language separates the motion into components, like the rolling and the down are separated by our language. Right? But that allows you because that way you would need fewer words to express all the different meanings. I mean, rolling doesn't go up, but there's other things that could go up, right? So as you, you know, jump up, jump down, right? Imagine if you had to have a different word for every combination of how you moved and which direction you moved. There'd be way more verbs needed, but instead you can sort of isolate the way in which you're moving from the direction in which you're moving. Right? So we kind of isolate those as separate words. That's what we mean by like language breaks up the world into its components, which allows us to sort of recombine it and express it in more ways. Right, yep, just said that. So. That also allows you to modify it, right? So you could say the rolling down, you know, you can sort of modify the rolling and not the down or vice versa. Does Nicaraguan, so one of the questions of this paper was, does Nicaraguan sign language do this? Is it these kids just made up this language, 20 years later, we're gonna see if they, it has this kind of property. So All right, I'll just show you these graphs. OK, so here's the, so here's. Uh Videos, basically just asked a bunch of people, either Spanish speakers. Or we're asked to kind of make a gesture, like, show me the ball rolling down the hill. And also ask the Nicaraguan sign language, be like, sign the ball rolling down the hill. Right? Now, what you're seeing is a Spanish speaker combining the path of motion and the manner of motion. So rolling down the hill is like this. It's combining. Path and manner, right? The sign language uh user. is doing roll and down as separate. Right? Like in English or any other spoken language, right, the two components of the motion have been separated into different signs. Well if you're just sort of gesturing this, you'll just be like, oh, it rolled down the hill. Right? So This study also then looked at. Uh, What if you were the first cohort of of Nicaraguan sign language or the second? So, all these kids, right, in 1980, Nicaraguan revolution, part of what they did is they brought all the deaf kids from all the rural areas, brought them into the city, put them in the same place. Right? Those kids started making up a sign language to communicate with each other. Then there's another batch of kids that come a few years later. Those kids show up and there's already all these kids signing to each other. That's the 2nd cohort. The 3rd cohort is like another, the next batch of kids that show up. Right. Now, this, this Y axis is the proportion of expressions in this task that like combined the two. That we're like, A, right? So A here is like, how many things were like rolling down the hill, and the B is like, how many things are like rolling down the hill. Now the people watching at home. Uh, won't get that part, but Think about A versus B here, yeah. Um. So what you're seeing is that the Spanish speakers who made gestures, 100% of the time, did this rolling down the hill in one gesture. Cohort one did that 70% of the time, but, and 30% of the time separated them. But cohort two and three were the reverse, where 70% of the time they had them be separate components and 30% of the time combined uh manner and uh direction. But there's more than one Senate, right? It's they didn't just do this with rolling down the hill, that's just the one example I'm I'm showing here. These were all children who were exposed to Nicaraguan sign language by 6. So, so the, so the first cohort, these were children who showed up at most 6 years old and then developed the language, right? There, this isn't looking at, so a bunch of kids showed up and they were like 12 and maybe didn't get to develop the whole language in the same way. All right. So Uh, all right, so that, so now when this was coming out, it was all very exciting, you know, and, uh, for people. But, and so we're like, so this is very fast, right? This is in a very small number of years, children developed a sign language that shows some key syntactic properties that are similar to languages that have been around for thousands of years. Uh, so, Was I? Yeah, Certainly, I mean, I haven't heard about it disappearing. I assume, yeah, yes, I would assume that there's people in Nicaragua who all who use Nicaraguan sign language. I haven't heard about someone saying it's like dying or something. It wasn't that long ago. How long ago was 1987 or whatever. Yeah. That's kind of long. Yeah, yeah, yeah. Yeah. Well, in language terms, it's not very long. Yeah. Um, and this was 2004, this paper, and so, you know, that's kind of like modern times. Um, for some of us who are alive. Then, anyway, so. Uh Right, alright, so all of that stuff so far is like all this stuff is in universal grammar. Now. There is a version, there's then. You know, cause people don't love to agree. There are people who are sort of nativists about language, but they're not putting as much stuff in there. Like that horn foot and light foot, what do they say? It was something like horn something and light, something, right? Hornberg and Lightfoot, something like that, right? And they were like, there's verb phrases and noun phrases and all these rules in your universal grammar. Some some psychologists were like, we buy that there's some basic things about syntax that are innate. Like we're interpreting, we're using abstract structures, but we don't think there's all that detail that they might have been saying about like verb phrases and noun phrases or something. But we still think kids are interpreting language and building abstract structures out of it. So how might you So what they were kind of arguing is that It's not as much as that, but they do say that there's some innate constraints based on how we might interpret our language input and have these biases to kind of put syntactic structure and link it with our semantics. At a minimum, children interpret language in terms of abstract classes, not just on a word by word basis. So we're sort of forming, we're sort of forming abstract categories of words from the beginning, like not necessarily like a noun and verb, but we're kind of forming these, we're, we're looking to form abstract categories rather than sort of imitate strings of words. Like an LLM does, but we'll talk about those soon. So, specifically, like abstract notions of agent, so agent is like the doer of an action, and the patient is like the receiver of the action. Right, so if I broke the vase, I am the agent and the vase is the patient. Right, so And so the idea is that. If we're learning about sentences and we're hearing people describe events. And we see that an event has two objects involved. Like, I watched, if I, if, if a baby watches me break a vase in my hand, I go, I broke the vase. It might go, well, there were two objects in that event. That probably means the sentence you said had two words for objects. In it, right? One of them is probably you, the breaker of the vase, and the other one's the thing that you broke. So the idea is that you would sort of look at this event, hear the sentence, and have these kind of constraints that like every guy involved gets a name. Here the vase is a guy. Right. So every object involved gets a gets a word. And this is like this kind, and they're sort of saying this is the kind of innate constraint. We don't maybe don't need all this other stuff, but we need this type of constraint to get syntax learning off the ground. So In addition, you, you, if you're thinking this way. You're going to very rapidly kind of recognize that there's systematic. Mapping between syntax and semantics, I remember that like the boy broke the vase, I threw the ball, right? The agent comes first, then the verb, and then the patient. And that seems to be true, very, you know, and that seems to be true across all sets. Now, the passive voice is when you flipped it, right? But in active voice sentences, the agent comes first, then you say the verb, then you say, so if you're kind of learning, if you kind of start out with this innate constraint that every object in the event gets a word, and there's also probably words for what the action is. Then from there, you're gonna start making these sort of understandings of like, well, agents come first, patients come second. Now How might you test this? Well, they were saying, well, you can test this by how children learn new verbs, because if you see a new event. And you know the nouns, and you know the nouns as order, you can figure out what the verb is. So if you never heard of breaking before, but you knew that like, if you said Micah broke the vase, and you knew that I'm Micah, you know what the vase is, you're like, well, the breaking must be that, I can sort of know that like you're the one who goes first and or whatever. You're gonna use that to like learn the meaning of break. So in this experiment, they did this with a fake, a fake. Verb called gorping. All right, so here, there is, they watched two videos, one TV had a video of a bear dragging a, uh, you know, twirling a frog around by its foot. And the other video is a frog pushing a bear's head down. Something like that. That's what that looks like. All right, and these are like 22 month old children who aren't speaking very much. Um, and not even quite 2 years old, 22 months to 24 months. And you say the frog is gorping the bear. And then, and then, so which picture is that? Right, well, It's the one, it's this one, right? Cause this is the one where the frog is doing something, the frog is the agent and the bear is the patient. In this one, the bear is the agent and the frog is the patient. Right, even though you've never heard gorping before, if you know that the, the doer comes first, and the thing that's gonna, who's like receiving the action comes second, you can go, oh, gorping must be this thing, and not this thing. Right. And that's what they found, where they said the frog is gorping the bear and all the kids are like looking at that screen. So in this you just got, you know, you just have two screens, and they just look at the thing that you think that they think you're talking about. And the kids were like, oh, the frog's gurping the bear, that's that one. Right? Was I? Why gorping? Like as opposed to another fake word. Why? What's the association with gorping? Is this actually like a new internet word that means something weird I don't know about? Oh, it sounds like groping, I see. Uh Yeah, that's fair. Like it was an innocent 2006 was a more innocent time. I don't know. Yeah, so. Uh Yeah, so. That, yeah, that was the peak of gorping. Right. So the sum, so the all right, so summary of these nativist accounts. Chomsky argues Hierarchical phrase structure and recursion is too computationally complex for general learning mechanisms. Thus they must be innate. Evidence that children never violate structural dependence and question formation. Supporting structural dependence as a as a sort of innate part of universal grammar. Deaf people not exposed to sign language before puberty never learn language, while deaf children never learn full syntax, while deaf children simply put in the same place as each other invent a new language. This suggests a genetically driven maturational sensitive period. Or it wasn't clear, the idea is like, if you can only do this under 7 and not later, that's a sign that like something has changed in your gene expression, right? Cause there's other cases, other animals where they're like, oh, like this behavior only can appear, only can happen if they're like exposed to it in the first year of life, and then they're not and it never happens. So this idea of a sensitive period is sort of from other animals. Early abstraction accounts assume less innate knowledge than universal grammar, but still posit innate biases represent language structure in abstract terms and link syntax with semantics. 00 yeah, sorry, you really need a break. I'm sorry. Let's do our 5 minute break.SPEAKER 0I. It's like the languages that you have. Asian or Asian. Oh yeah, yeah, the Asian is different. Like the order is different different like, but like sometimes you know I do trend it's hard. That actually makes this. But. Yeah, so there, there are other examples, right. No, it's true, right? So these are, I've heard in some languages that. I mean, it also, if you think about, um, afraid, if you think about fear, right, like I fear thisSPEAKER 1thing, but like I'm, oh wait, like it scares me, right? So if I fear this and it scares me, itSPEAKER 0like kind of, you know, so there are examples where it kind of swaps around. Um, but right, so, so it's not like a 100% rule, but if you were to hear a new, I think they're like, if you hear a new one, you're gonna go along with reading what is like the most common word, but it's also the case that there, soSPEAKER 1there's also, I mean, other languages like Finnish are called like a free word of a language, and you kindSPEAKER 0of put them in any order, but, but it's because they have. Little inflectional morphine or they have little suffixes that tell you what's the agent in the patient, right?SPEAKER 1So, so imagine if you had like, you know, dogSPEAKER 0doggo, yeah, so, so if you have, so if you have like doggo what the dog is the subject. Like dog off when like dog is the object you can kind of have it be like either you couldSPEAKER 1have it go anywhere because you're paying attention to likeSPEAKER 0the morpheme the little oh there's on which it is so right so English uses a lot of strip hasSPEAKER 1right so, so, so part of how that works isSPEAKER 0is that yeah, yeah, yeah. Right, that makes sense. And my other question is with that sign language exam here, this like wouldn't practically have to yourself, but say like to um say like an adult problem when you're going to. I And then have a child and instead of like language with the child, yeah, so, so that, so theSPEAKER 1assumption would be is that that would be if theySPEAKER 0somehow weren't exposed to the, the assumption would be thatSPEAKER 1would, that would be like that, that, that, that those cases, those sign language cases where, where the assumption would be the adults are kind of doing it. A kind of bad job but not terrible and thenSPEAKER 0the kids would get better at it.SPEAKER 1The assumption would be be like that devil kid example, but of course in this case you'd be surrounded bySPEAKER 0all these other speakers and so it's like you would, you would learn it because you would just have all the other kids speaking it as well, right? Why does like second people that learn a second language in adulthood. Well, so, like, so, well, like, well, if they're exposed around it all the time, you kind of, if you'reSPEAKER 1immersed, you can sort of it is that when an adult kind of gets moving. It's like, one is you still have a you like are maybe using a different pathway you use all thisSPEAKER 0explicit, but it's a much more explicit instruction, right?SPEAKER 1And you can also do kind of analogies from yourSPEAKER 0own language kind of translate right in a way thatSPEAKER 1if you're like, if you're, if you've never been exposed to language, then you're like Mary no name shows up later. There's not like some other language that you're gone, soSPEAKER 0it's kind of its own thing. Very cool. New York Island is cool, right? I've never heard of. Yeah, yeah, it's like a, yeah, yeah, it's like a,SPEAKER 1it's like one of the few guys cognitive scientists discovered something, you know, it's not usually we don't really discoverSPEAKER 0anything. They're like, oh, look at this, it's like, yeah, yeah, yeah, yeah. Oh yeah, so yeah, I. Uh, yeah, I'm pretty sure, but I don't know. Yes, yes. OK. It's still 20%, but it's only coming out of 8 quizzes rather than 9, so your score probably 8004. No problem. Yeah, yeah.SPEAKER 1All right, so yeah, so we will, we'll, we, this will bleed over into the next time, but again, That's all right. Instead of next time being extra short, you just get some extra. Uh, stuff here. All right, constructivist approaches. I mean you guys think I want to read more. Here's some, uh, I don't know why I say that. That was a In case you wanna see, uh, some people write about the things you're like, oh, I wonder, did the same mechanisms happen in language development as in conceptual learning? You're probably thinking that. In case you want to see that, you could read some papers on that. Um, right, so, All right, now we're basically gonna go through all of these points and sort of counter them with constructionist theories.SPEAKER 0Do you know when the time Like Right, where you'reSPEAKER 1like, here's the nativist claim, and here's the constructivist claim, and it. You could probably upload the slides and get Chat GPT to do it. Yeah. Um Or co-pilot or whatever we're supposed to be using. I mean, that's, you know, the Microsoft one sucks. The university has this contract with Microsoft, uh, anyway. So, so Um So All right, what's the constructionist approach? Let's start with the, the thinking. Well, all right, yes, clearly humans are endowed genetically with the ability to learn and create language. If they weren't, then we wouldn't have la, right? Cats do not have a lang do not have language, despite our loving of them, right? And humans do. The difference between us and cats are our genetics. But it's also true of many things like baseball. Right? Cats don't play baseball. Right? You know, maybe they, well, they, we, we don't know what they're up to at night, maybe they are playing baseball, yeah, so. But also, but no one goes around being like, there is an innate universal baseball. Right, that wouldn't make sense. So, there's all kinds of stuff that we do that only humans do. And also an English guy, he made this argument, it's not just because I'm American, why you chose baseball instead of cricket, I don't know. Anyway, um, do we maybe want to point out how weird it was? Yeah. But so. But the issue, if you, if you recall our earlier arguments about the constructivists versus the nativists, it's about domain general learning mechanisms versus specific learning mechanisms, right? We talked about this with the objects in math and number, right? Are there, uh, you know, do we have knowledge about, are we, do we have a module about object knowledge that then helps us learn about objects? Right here is, do we have something in our genes expecting there to be language, right? Cause once you expect there to be language, you have something innate about it, versus all we have is this brain that can just take in information and learn stuff from it in a sort of powerful way. And we don't know anything about language until we're exposed to it. So the constructionists would say, No, we have general learning mechanisms, and we somehow get syntax from that. We don't have any genes that are like for syntax, specifically. I'll bring assumptions, we do not start out with any endowment of pre-specified abstract medical categories and phrase structure rules. So the input is actually, so a lot of the things that they tell you, a lot of the things, so there's, I didn't even use this phrase, but a lot of the nativists and the linguists will say there's a poverty of the stimulus, as in, there's not enough information in the stimulus, right? When, when I, when I gave those examples of 1, those sentences, these are all the sentences that are allowed in language, and these are a bunch of sentences that aren't allowed in language, right? What the, what, when they would say that the stimulus is impoverished is that the stimuli isn't clearly telling you. Which are allowed and which aren't allowed. There's not enough in the, there's not enough in the input. Constructivists actually might analyze the input and go, there's actually what you can actually do way more inferencing from this input than these guys are saying. So you can't just say you have to posit innate constraints because there's no possible way to learn this stuff. We actually can talk about how it is possible to learn this stuff and. You know, people have made these arguments updated with things like chat GPT, right? So, part of it is when Chomsky was sort of writing his book review of Skinner, it was 1957, and at that time, learning was associative learning. Right? This was sort of pre-quote cognitive revolution. Now we know about all kinds of stuff about how children learn stuff that we didn't know in 1957. And the argument is that these mechanisms, many of which we've been talking about for the last five weeks, can explain syntax learning. So we're going to talk about 2 in particular. Um, statistical-driven learning where the structure of neural networks. So this is the type of stuff that Les Cohen and Linda Smith were talking about, and analogical and relational abstraction as in the getner work. Also, there's a lot of emphasis in this, on, on our social abilities and theory of mind, but I'm not gonna make those connections. But also one of the things that Mike Tomasella points out, remember, remember all those lectures on how humans had theory of mind that was like different from other animals? Caroline talked about that, right? Right, so it's, you know, and that the argument from that goes. To understand what each other are talking about, we have to kind of attribute mental states to each other. Right. So imagine I'll give an example of like a point, right? If I'm pointing over here, this isn't language, but if I'm sort of pointing at something, so let's say you're looking for your keys. And I see you looking at your, you're you're, you're like searching around for your keys and I like point over there. Right, you might go, oh, maybe my keys are over there. If a bird lands and its beak points over there, you don't go, I bet the bird is helping me, trying to tell me where my keys are. Right? Cause we don't attribute mental state, those type of mental states to birds, but we attribute those mental states to each other. And so to communicate with each other, we have to attribute mental states to each other. Right? And so our theory of mind, as our sort of more advanced theory of mind is sort of necessary to do any communication, let alone, you know, of the kind that we do. Like, that's why, that's one of the reasons why primates, Mike Tomasello would say, it doesn't matter that I mean primate, how would primates develop syntax where they don't even point at each other, which they don't. They had a demonstration we had it, but he had like a, uh, like a big like chuck roast, like under a trash can, and there's a gorilla standing there. And he's just like pointing at the trash can. To be like, is this gorilla gonna figure out that, like, That that's where the chuck roast is, the gorilla like wants the chuck roast, right? But the gorilla is like, it's the gorilla is treating you like we would treat the bird searching for, searching for our keys, right? It's, you're not trying to help me find the food. Right. Anyway, so, so he would sort of say, you, you syntax, you can't even understand, you know, you need social reasoning to get to do any of this stuff. All right, but I will talk, now it's now assuming that we have theory of mind, OK. Now, how might you learn syntax, given that we are assuming that we're trying to communicate with each other and tell each other things. One mechanism is statistical learning. Infants and children will track the transitional probabilities between sounds, words, and phrases and how they are distributed more globally to learn grammatical categories and phrase structure rules. First test case, infant word segmentation. So this isn't syntax, but this was sort of looking at a mechanism that they think could be a part of syntax. What's the transitional probability? The probability that after a given syllable, let's say, a particular syllable will occur. Let's imagine. And this this doesn't have to be syllables, it could be words, could be phrases, but for now let's do syllables. Let's pretend. And this is not true, but let's pretend. That the, the word, the, the sound pre only ever existed in words pretty, predict, and precise. Right? And half of the time. That you ever heard the sound pre, it was followed with T, as in pretty. 225% of the time, it was in the word predict, and 25% of the time, it was the word precise. If that were the case, Pre, there was a 50% transition probability from pre to T. All right, now. How might, let's say that, let's say now, it's not this dramatic, but if you look at the statistical, if you just record speech. That people, if you just wrote a chorus speech, you could start doing things like what probability does any given sound occur after any given, any other sound. And there are, it's not evenly there's not a uniform distribution. Some sounds co-occur with each other much more frequently than other sounds. And this might be a way that kids start learning how to sort of segment the speech stream. So, if you hear a string of words such as Pretty baby You could potentially use the transitional probabilities to find the words. So for example, if something is a word, those syllables are much more likely to co-occur than when two syllables are not in the same word. Like, because every time, you know, when you, when something's a word, you say the whole word like pretty, but often pretty doesn't come before a baby. Right? T comes after pre more frequently than pretty becomes before baby. So the transitional probability within a word is much higher than the transitional probability of sounds in between words. His words can sort of go after each other in any potential way. Now, do children, are, are children exploiting these cues, these statistical cues. So in this artificial grammar learning experiment. They just heard a series of sort of roboticy sounds like bidaku padoti gola bu bi daku is what they just heard for like just 2 minutes. If you are an 8 month old infant sitting in a chair, listening to Baku bato di Golabu Baku. Right, and There were a small number of words in this fake language. There was a None of the words shared syllables. So, so bedaku, all different syllables and podoti. Right? So So Da came after B 100% of the time, K came after Da 100% of the time. But Paul came after Kup only 33% of the time, cause it could have been one of, if there are 4 words total each and you didn't say repeat words, or maybe you repeated words in 3 words total, it doesn't matter, but. You had like a 33% chance of coup and paw going, cause it could have been coup and go. And not coo and pa. Right, so then there's 100%. So sounds follow each other 100% of the time, or they follow each other only 33% of the time. And there's also no gaps, right? There wasn't Baku, Padoti, Golabu, Baku Padoti. So after listening to this continuous stream of speech sounds for 2 minutes. They then kind of had a speaker. And they sort of looked at like how, how much they like kind of oriented towards the speaker, sort of seemingly interested in it. And it would either now there would be gaps. It would either be words like Baku. Baku Baku or be non-words, kupado kupado kupado cos those combined words. And weren't actually words. All right, and so in this study, the infants. Showed a novelty preference for kuppao. Over a big Dau Right, because, so suggesting that they understood, they kind of had parsed the speech stream into these words. Right. Um, right, so. is absent Yes, that's a good point. This, they made it, this was the digitally created and there was no stress. So they also removed that cue. This was, so it was just isolating, that's why like Baku, padoti, Golabu, and I, when I was saying Budaku, Baku, that was sort of my own implying English, applying English rules, but it was sort of robotic and, and no stress differences. Yeah, so they tried to isolate that variable. Good remembering. Uh, so. Some other examples of how physicalal learning could work, and people are like, OK, could this learn grammar? So how grammatical categories, you could look at child directed, you can analyze child directed speech. So for example, these are all statistics, these are all, none of these are deterministic rules, but they're probabilistic, right? The word after the is typically a noun. Sometimes there's an adjective there, but usually it's a noun. The word in between is and ing is usually a verb, is jumping, is throwing. Often the blank is followed by is blanking. So we're just sort of looking at these, so you can start doing these statistical learning models. These are sort of pre, now again, these, these, these are, these physical learning models are pre-modern. Things Right, these are sort of much older statistical learning models than LLMs. These would be small language models, not large ones. And these models could, looking at child-directed speech, could sort of start categorizing words based on these types of statistical cues. So the, so the sort of theory was, was that maybe if you're doing all the statistical stuff, you can start, there's enough information in the input that you still, you could start abstracting out all these grammatical phrase structures and things like that. Now, Some people took, OK, said, all right, let's imagine this is how you're learning, you're learning from these statistical, you're learning it from language statistics. What about that structural dependence stuff and the never ever asking a question that way? Right. The constructionists claim is that the learning grammatical structures can be driven via these sorts of mechanisms. Can they re-explain Crane and Nakayama and explain how A child would never say such a thing. Reminder, given the man who is smoking is crazy. I think I said the boy before, it doesn't matter. Children always turn into a question by respecting phrase structure, as in, is the man who is smoking crazy and not is the man who's smoking is crazy. However, If you, let's say you were kind of learning little chunks of words. And you were learning how to string them together with statistics. And you're building up bigger pieces and smaller chunks. They would argue, well, You never say who's smoking. Now There are dialects of English where where you drop the is, so you might say who's smoking. But let's Um, you know, ignore that for now, right? Like Irish English, African-American vernacular English, some dialects where the, the is, is dropped. But let's just take kids who are not being exposed to those dialects. Right, where, um, is the book, so you were never, so if you were kind of learning statistical contingencies between words. And you would only say things that kind of reflect the physical tendencies that you've heard. You would never say who's smoking. So why, so this, so no one thinks even about Universal grammar that you would say this. OK, so how can you tell the difference? So if both theories predict The same thing, it doesn't really help you. Right? However, Ben Ambridge came up with a kind of study to distinguish these two things. Now, unlike for his questions. Can questions do something different, right? So if you say the boy who can smoke can drive. Right now when it's can. The boy, so here, right, so, so when it is the boy who is smoking, smoke is in, but when it's can, it's just smoke. Right? Probably didn't think about that previously, but can the boy who can smoke drive is the correct thing. The in the structural dependence errors is can the boy who smoke can drive. That is an error. But who's smoke, unlike who's smoking, is. A grammatical pair of words in other contexts. For example, people who smoke die young, right, who smoke is. A word is a combination of words that children would have heard before. And again, other verbs as well, I'm just saying smoke, right? Uh, this isn't all about that one verb, right? But so you, while no one says who's smoking, lots of people say who smoke. OK. So, if children actually are just kind of stringing together words. That, you know, from what they've heard, shooting together sentences, recombining stuff they've heard before and not following some universal grammar, you might make errors. With canned verbs like this, like you might say things like, can the boy who smoked can drive. So they use the same elicited production method. Uh, where they make seven year olds are asking questions. OK, what did I say before? I said they're 6 out of 7, sorry, slide nos. So. Um Unlike with is questions, where no one made structural dependence errors, with can questions, they did. On average, makes 7% of error, 7 structural dependence errors, where children ranging from 0 up to 43% of such sentences with a structural dependence error. So here, Right? So sentence questions like can the boy who smoked can drive. 7% of the kids' questions were like that. Which isn't a ton, but it's definitely more than 0. Also, universal grammar would never would not think, would it just wouldn't have anything to say about things like word specific effects like this, where your syntax, your, your syntactic competence varies with the verb like this. But if you're building up your syntactic knowledge from exemplars. And not from sort of starting out with abstractions, then you would get all these sort of word specific differences in how productive they are. All right. So, that's the res it.SPEAKER 0What But I mean, is versus right.SPEAKER 1So the, so the main verb, the smoke, the smoke, and not the exi so it's called auxiliary verbs like is and can in this context, like can smoke is smoking where the, the sort of primary. Sentence meaning is being communicated by the kind of smoke first or drive or whatever. Uh, right. And so it applies across all of those, but there's word specific in the sense that the particular auxiliary verb that you're using like is versus can or should. So a lot of these, these type of words are the, will function in these auxiliary roles, but the, the, their, their product, their competence seems to be related to that, those specific words of the, of which auxiliary they're using, not like which main verb they're using.SPEAKER 0Is that Like a, that's like a universal.SPEAKER 1Yeah, but no, sorry, but the, but the, but the key thing is, is that when they're eliciting the questions, they're making this kind of error where they're taking the can from in between the who smoke and moving that to the front. So they're making this kind of structural dependence error, like, um. Where, when it was is, if it's is the boy, you know, if it's. The boy who is smoking is crazy or whatever, no one ever takes that is, go to the front. They would always take the second is. But when it's can, they'll take that first can and move it to the front. Without Yeah, so, so here, they're kind of violating phrase structure rules. Cause if you say can the boy who smoke can drive, you're violate because this who can smoke is a modifier of the boy, right? So the so the noun phrase is the boy who can who can smoke. And they're, and you're taking the, the verb out of that and out of that noun phrase to the front of the sentence, violating the structure of, of the sentence, where this one's the main, the second can is the main verb of the sentence, because it's like the boy can drive. Right, so if you said the boy can drive, you'd say, can the boy drive? But we say. The boy who can smoke can drive. They're kind of messing that up and moving. That first can over in a way they never would do with is. Is that, I realize it's hard, there's real working memory load with like having all these words like this, um. I'm not sure how else to Uh, uh Because Right, and structure, yeah, yeah, yeah, cause the, the, the rules are operating at the level of verb phrase and noun phrases, right? It shouldn't think, oh well, it's can. I have a different representation of syntactic rules for one auxiliary verb versus a different auxiliary verb, such that the way I'm making these sentences are gonna vary. Right, it's sort of like, it's operating at this abstract, the universal grant was assuming operating at this more abstract level. Yeah, yeah. Um, Yes. All right, so how does, how, how might this work in addition to Statistical things. Um Right, so abstract medical categories, instructions allow for free production of words independent of how individual words have been used before. However, this view of gradual abstract, rather than assuming that kids are starting out, forming abstractions like as rapidly as they're being exposed to language. He's saying, well, it's actually much more gradual than that. We're starting out use children are actually much more imitative when they're when they're first learning language. They're only kind of using words in ways they've heard before. And then they start gradually becoming more productive. They start slowly noticing the commonalities and patterns of words used and generalize individual words to novel context. In the way that's similar to how Gentner proposed the hypothesized learning mechanism, the, the analogical stuff. Children's earliest grammatical structures are based on individual words and more abstract adult-like grammars are built from there. More specifically, let's say you hear two sentences like, I kick it, I kick the ball. You might go, oh, this sentences kind of work with like I kick and then the other of the sentence. In the meantime, you've also sort of made a little schema for I hit objects. Right? And then you might go, oh, it turns out lots of things are like I, then an action and then an object, and also mummy. I also noticed that anytime I talk about mummy, there's like an action and then an object. And eventually you kind of realize, oh, there's like a subject, a verb, and an object. So the idea is that just by sort of noticing all these commonalities across language usage, you're just gonna start noticing all the, all these, all these generalizations. Where you go from like very concrete, I'm imitating individual words to slowly discovering these abstracts and tactic rules. It's a process of schematization. And the idea is that you're creating verb-specific schemas like X kiss, kick Y, P kiss Q, and then from there, it goes up to, uh, you know, eventually, noun, action, other noun. Uh, now in V back, you know, now. It's OK, what's the evidence for this? This is how he thinks he's doing it. We're not, we're, we're, we're, we're not doing this thing where we're starting out as abstract as possible. We're kind of slowly building abstraction. I'll do one, I'll do this experiment and then we'll do the, the squares quick. All right, so novel verb studies. Teach children a novel made up verb, like corping, but this time they use teeming. I think, damming, teeming is the word. Uh, teach children a novel made up verb, meeking, tamming in one construction, and see if they use it in another. Right, so if they have all these abstract, so lots, so for example, lots of verbs you can use in more than one syntactic way, right? You can say, I bounced the ball, the ball bounced. Right? You can say, you can use, you know, bouncing works in both I bounce the ball and the ball bounced. So if you were, had general rules, if you learned a new verb, you might go, oh, I heard it one way, but I also can just use it in this other way because I know that's allowed, cause I have an abstract grammar. What they sort of did was they said something like, Something equivalent to I bounce, you know, they said the sock is tanming. And they tried to elicit. There's some people I'm gonna, I'll just hold off on more linguistics lingo. So, the sock is tamming, huh? So what's they're doing is then Big Bird, right? So they're like the song Big Bird waving the sock around on a string. And you're saying, and you're going like, the sock is tamming, what's Big Bird doing? And then the kids should say, Big Bird's tamming the sock. Right, if you knew how verbs worked in general, you'd be totally happy to say that. Right? However, At 2 years of age, only 3 of 16 children, after hearing the sock is tamming. Then said, Big Bird's tamming the sock. 2.5, 7 out of 16. And then for 4 year olds, it's like basically all of them will do it. They'll all go, oh yeah, Big Bird stamming the sock. So the idea is that 2 year olds are still at this kind of word by word specific grammar phase, and then by 4, they have like an abstract productive grammar, but at 2, it's still their grammar is still tied to individual words, is what individual verbs specifically. And they also would, they also would say, but it's also verbs, because if it's a noun like this is a Toma, like here's a new object, this is a Toma. They'll go like, OK, give me the Toma, I want the Toma, I see the Toma. So when it's a noun, they're happy to say, so it's not like a shyness and a not wanting to ever say something that the adult didn't say. It seems to be specifically verbs and syntax are sort of tied together for for young children. Uh, let's do the quiz. Did I make it? Oh yeah, here we go. What distinguishes what what distinguishes sign language from gesture? A, sign language is compositional. B, sign language conveys meaning. C. Sign language reflects our innate communicative abilities. All right, we are 100% correct. 1 out of 1. Great job. So, I guess I could have went. Yeah, sorry, 303, even better, yeah. Right. That one was. What was that? Oh, compositional, I guess, yeah, sorry, compositional is the breaking things up into their components. Right, so then you recom then you compose them, right? So that's the like the role that, right, the role, the, the, the, the rolling is separate from the down. It's like composed into, you know, you break it into components and recombine it, rather than sort of holistically expressing things. If you think about it like, an alphabet is a compositional system, but using like pictographs or like is like not, cause it's like one image for the whole. Thing, right? While the letters are like these, the, you know, the words are made up of these little subcomponents. All right, I got a thumbs up. Um, I guess we can. Whoops. Should we just, hold on, what's a good point to, yeah, so we'll finish, I guess we can, I don't know, we can, I don't know, well, a couple more minutes. Oh yeah, let's do a summary of this sport so far. Claim 1, children can learn syntactic categories through the distribution of physical patterns of words and their input. Claim 2, children's initial syntactic knowledge is item as in like based on individual verbs, and abstract constructions, abstract syntactic constructions, syntactic rules are formed slowly by generalizing across these individual things. So next time. We'll counter the last two nativist points, and then we'll talk about culture. And, and we won't finish, we'll still, we won't, uh, finish late on our last day. So has everyone finished their essays? Yeah. Sorry, sorry, that was. Like, one day you won't have to write any more essays. Yeah, that's not true. I write, I just do it, I guess eventually you might get paid for it. So yeah, it's not that you don't have to writeSPEAKER 0anymore.SPEAKER 1Uh.SPEAKER 0What's that? Yeah, no, no, it is, that is, yeah, right, noSPEAKER 1one reads anymore. That's, that's another, that's a general problem. I, yeah, there's something like 20% of people under 20 say they like have ever read a book for fun. It's pretty grim. You think it's a lie? You think, you think more people, you guys read books? Well, that's very good. Look, we're all us middle-aged people are very concerned aboutSPEAKER 0this. Oh Uh, I mean, well, some of these stats areSPEAKER 1in America and that place is horrible.SPEAKER 0I don't know. I'll see you later. Sure, that's all right.SPEAKER 1Hold on, let me throw this.SPEAKER 0Uh what?SPEAKER 1Stop that. Sorry.SPEAKER 0Why is this not letting me? What? Oh. Delete. Yes.SPEAKER 1Oh, ha ha, I didn't read that properly. Sorry. OK. Yeah mate, no-one reads instructions.SPEAKER 0000, yes. All right, there we go. Yeah like uh. Yes. Oh, she actually has been meetings schedule I I. OK cool. I get into like, yeah, yeah, and just say that that, yeah, and then I go. and, like children, teenagers and adults, yeah, yeah, yeah, youSPEAKER 1can totally, yeah, yeah, yeah, it's, yeah, sorry, it's totally up to you just to do what you think captures it best. Like what do you think best captures what's going on, right? Yeah, it's not like you have to do it one way or the other, but would you advise that seems,SPEAKER 0um, no, that seems good.SPEAKER 1I mean, you know, you have a, you have a word limit. But it's like, but besides that, it's, it's, I think that's good to do. Yeah, that's, that's sort of what the exercise because the writing of the intro is that, you know, I kind of set up the intro for everybody in a certain way. It's not like that much to, to arrange to do it, right. So really it's all, it was all about what that type of stuff. So you should include this at all, yeah, OK.SPEAKER 0Thank you. So yeah, all good. I do, yeah.SPEAKER 1Sure, let me just double check that like I've signed out properly. Wow Uh, yeah, yeah, yeah.SPEAKER 0And his language acquisition is completely different. Yeah. I feel like he, he gets language, but it's not. Yeah. It's like Yeah I I mean, it's so, so it's probably right. It's, it's definitely more like like. But it's not.SPEAKER 1No, I mean, right, I mean, this, you know, this is clearly, so I actually don't know a lot of.SPEAKER 0Yeah, yeah, but there's a huge, so you know that the people in communication sciences. What's it called? What is it called communication science and disorders. Sometimes I do, but there's like a, there's like a,SPEAKER 1there's like within the faculty of health there is a communication sciences or people who like other people who likeSPEAKER 0train people to become these therapists and stuff like that. Right. And so there is a lot of work on whatSPEAKER 1language ASD is, is like, right? And you will probably have some kind of weirdo linguistsSPEAKER 0like doing like, oh like this is how this is part of the universe, you know, you know, so yeah. But Just the OK My Wait, so they can't trackSPEAKER 1cause I mean there's a certain amount that.SPEAKER 0Oh right, yeah, yeah. It's like a Yeah, Yeah, no, no, totally. I mean, I, yeah, I don't actually know, I don'tSPEAKER 1know the prevailing theories for those particular patterns.SPEAKER 0I mean there are, there's also a lot of work. There's something called like a language specific.SPEAKER 1There's like, there's certain types of language deficits that it seems like the kids are sort of totally normal cognitively,SPEAKER 0except it's called like special language disorder or something likeSPEAKER 1that, but you have language problems, right?SPEAKER 0Which is different than ASC obviously because there's like other things ASD, right? The only if you were indicted AFC and the only issue was language problems, right, but there's a, there is a lot of work in this area to understand what is happening in these cases.SPEAKER 1But it's hard to because, but there's so many factors that go into this, right, because I know there's some work on sometimes kids have different ISD kids may haveSPEAKER 0more memory problems, right?SPEAKER 1And, and a lot, and pronoun tracking is very. So you actually might see this with older parents whenSPEAKER 0they stop being able to track when you say like it and they're like, what, you know, they have, they have trouble like tracking like what the pronoun was it. Because, you know, it's working memory though to be like, oh, the last object you said was this, and I'm connecting that to it right now. That's a bit different than saying me versus I, and these are adults who wants to learn, right? And so, so the, so the combination of other cognitive things include and, and if you relieve the kind ofSPEAKER 1social. You know, the social aspect of it, and also partSPEAKER 0of this is the social deficit. They're also not like tracking their, your communication on the same there's, there's so many people have this idea that kids with ASD are sort of less just just find social interactions that's rewarding as they try to find face to face interaction that's rewarding. And, and these types of things, right, and so if you're not kind of really looking at someone's faces or are, it's just harder to, you know, because you're not just getting it, you're not getting nearly as much focus and attention on, on money, that's right. And so all these experiences that need that need that you build up over time, you're just getting way less.SPEAKER 0It's Yeah Yeah. Hi everyone.SPEAKER 1All right, so we are. 5 minutes passed, so we'll begin. Come on guys, you gotta pay attention. You gotta be a good example for our uh guest here. When you get to uni, you can't just talk, you can't just talk while the lecturer is talking, yeah, so. Um, yeah, so, all right, so last time we were talking about the constructivist, and nativist accounts of language development. We're gonna finish that up and then move on to the sort of final topic on, uh, cultural effects in cognitive development. So All right, you might have recalled from. If you remember. The nativist part, we gave evidence from for nativism. Or at least an account of nativism that allowed for children to be very rapidly forming grammatical abstractions. Uh, I don't have time to recount that entirely, but hopefully that just connects to what you remember from last week. Um, that children, and then one way they did this, right, was that they reasoned through the evidence of, um, early verb learning, right? So you recall. Do do do this experiment, the gorping experiment, right? So when we had the frog is gorping the bear. Right? The gorping is a verb that no one heard before, and there are two events happening, two animations happening. One is the frog is acting on the bear, and the other one, the bear is acting on the frog, and the and the 21, 21 month old children would look at the frog as gorping as acting on the bear, suggesting they understand that in general, the sort of doer of actions become, come first in the sentence and the receiver of the action goes second in the sentence, or after the verb in English. Right, and the idea what, and this is a, this was a sort of rebuttal. Remember I was talking about the idea last week in the in the constructivist account that with children when they're learning grammar, it's very tied to specific verbs, right? And they, they haven't formed abstractions, they're very gradually finding abstractions, and then these people said, hey, look, if that were true, then they wouldn't be able to do this. They, they, they'd be at, they would just be looking randomly. At either event for gorping, but the fact that they look at this one shows that they understand that in English, the, you know, the doer comes first. So constructivists then looked at that paper and replied to that and said, well, if you look at the warm-up part of the. Task, right, so before they got to this bit, right, so, so in this experiment, there's always a sort of warm-up with child experiments to make sure that the child is sort of like understanding what's happening. Right? So here they're showing them the two different screens, they're saying the frog is grping the bear and the kid's looking at the one. But they're, they set that up with a, with, with some familiar verbs, where they sort of where they have two different TVs. One is like, look over here, the frog is like washing the wall or whatever. Look over here, the bear is like jumping rope. So they're sort of getting them into looking back and forth at different screens to then set up the situation where both screens are on and they have to look at the one that they think is the one being talked about. So this Dittmar at all paper. Set it up, noticed that, well, a lot of the syntax of the warm-up, kind of primed this interpretation. Like the frog is washing the bear, right, in the sort of warm-up. But if you have a weirder syntax in the warm-up, right, that's like, this is called washing, it's kind of a weird thing to say, you wouldn't ever like look at someone like washing something and just say like, this is called washing, but it, you know, it allows them, the kids to get used to the idea of looking back and forth at the different screens for the different events without um. You know, uh, uh, look back and forth different screens, different events without kind of using that same syntax. And basically what what they basically showed was. When you had the warm-up not containing the same syntactic form, they no longer looked at the frog gorping the bear, when it was said the frog was gorping the bear. They no longer systematically looked, they were at chance, they kind of looked at both screens equally. So they were sort of saying in that moment. They were able to kind of like generate, you know, to kind of extend from one verb to the next in the same syntax, and once you broke that connection, they couldn't do it, suggesting that they don't really have this abstract syntax at this age. So, alright, so that was one point. So, so the constructivist point, what we've been doing is showing how for every point of nativism, the constructivists have some way of trying to like debunk it or something like that, right? Now Uh, and if you don't, you know, if you play it back. The report recording from last week and then play that part, it should all make sense next to each other in case you don't remember that other experiment from last week. Um, the next point was sensitive periods, right? And if you recall when it came to sens, some of the evidence for sent now. In naturalistic evidence, right, for for Nicaraguan sign language, right? So you recall for Nicaraguan sign language, that the the data was when the, when the deaf children were brought together. All right. And they were under, they were 7 or under, they fully learned the syntax, right? But if but if people, the deaf people were brought together and they're already over 13 years old or so, they never really acquired syntax, the full syntax, right? Remember that? And now that's a naturally occurring event. It's hard to sort of explain what the mechanism is of that, right? It's not an experiment. They sort of showed, well, there seems to be this sensitive period. Now, of course, much now with nativism and constructivism, the constructivists aren't necessarily denying that there is a sensitive period, but there's different explanations of like what that sensitive period is, like what is changing in our brains, right? What is changing in our brains to make us able to sort of fully acquire syntax of a of a first language before puberty and what is changing after puberty. Alright. Now, if you also then recall the the experiments on children over-regularizing, right? So you had, you had the um artificial grammar and the deaf children of hearing parents. Over-regularizing syntactic rules, right, while the, while the, while the, while the adults would not, would just sort of match the probabilities of what they saw, right? So this was a situation where the, the, the, the, the, the hearing parents would be sort of signing, right, and they'd be accurate in their syntax 70% of the time and kind of randomly doing stuff 30% of the time, right? And the children saw that and, and, and went back with them. You know, more accurate than they than they saw, right? Just to remind you of that pattern. Now One of the things that people have argued is that children. Overregularized, not just in language, but like in general. This isn't a language specific phenomenon. Now, so you might think this is kind of a stretch, but this is sort of the logic, right? So if you're an experiment and the goal is to flip light switches to turn on a light bulb, OK? And let's say across across trials, you get one shot, right, you're like, OK, which of these three light switches turn on this, turns on this light bulb. And it sort of fluctuates across trials which one is which. If one is done 65% of the time, one is done 25% of the time, one's done 10% of the time. The best strategy is actually doing the one that says 65% of the time, 100% of the time. Right, because, because it's random which one, so it's random like, there's no pattern, right? So it's 65%, 25%, and 10%, but you don't know which one's which. And so you're actually, so if you. Try to guess, like I'll do this 1 2/3 of the time and this one on 25% and this one on 10%, you're most likely going to guess wrong, which is which, it's gonna end up being accurate, most likely less than 65% of the time. Now, part of people doing playing slot machines is they don't really think this way, right? So, um. You, you, you, you start believing that there's patterns that aren't there, right? And you go, oh, that one happened 4 times in a row, I'm gonna do a different one now. But they really are random. If you're an experimenter, you can control this, it's completely random behind the scenes, besides these proportions. Uh, so kids do like a quote, the smart thing, where kids are more likely to sort of do the frequent one like 90% of the time. Where adults will do like perfect probability matching, where they go, this one happens 65% of the time, so I guess at that time 65% of the time. This one does 25% of the time, I guess it's 25% of the time. I mean, it takes a little while to get exposed to do the task for you to realise what the probabilities are, but once you kind of have an understanding of the probabilities, adults tend to match and they tend to be wrong a lot. While the kids are more likely to maximise, which is the right strategy, of doing the frequent thing much more frequently. Now, this isn't because children are smarter in the sense that they have a better understanding of probability. There's actually an explanation that their reduced executive capacity leads to this, because one of the things that executive function helps to do is is interpret is resolve conflicts between resolving between competing information streams, and one way to do that is via probability matching. It's kind of if one thing is really salient and something else is less salient, kids sort of lack of ability to sort of deal with that properly just means they do the salient thing more. But it turns out when the noise is, when the world is really random, doing the salient thing more can be advantageous. Right? So there's sort of, there's some people saying that actually why there's a sensitive period is because their frontal lobes aren't developed well enough yet. That for certain types of learning, it's actually better to have a reduced executive function. That's the argument that now. You could say there's kind of a stretch there, taking data from probability matching and explaining like Nicaraguan sign language, you know, you could sort of see, you, you could sort of think it doesn't quite stretch, but on the other hand, it's not like we have a very clear genetic language explanation for the sensitive period either. Right, so we sort of all agree that it does appear to be this sensitive period where if you're not exposed to a native language, uh, by, you know, early puberty, you may never acquire it, but there's just different explanations for how that's happening, right? Is it a, is it a general brain effect? Is this because the brain in general is changing, or is there something specific to language that is changing, right? And the Echomskyian view, that view is like, this is a very specific genetic effect. Is that Makes sense how this logic works?SPEAKER 0Yeah. Yeah, exactly.SPEAKER 1Well, in the sense of the probability, so, so here it's just literally like. So kids would be like, all right, so, so if you have 3 different options, so imagine instead of light switches, those are like 3 different suffixes to verbs, right? So let's say the right and, and let's say adults are are, let's say you're exposed like like the artificial grammar experiment, right? Let's say you're exposed to one suffix for a verb 65% of the time, another suffix 25%, another suffix 10%. The adult will go, OK, I'm gonna produce the 65% suffix 65% of the time. I'm gonna try to juggle them all, while the kid is more likely to go, I'm just gonna take that one that was 65% of the time and do that like 90% of the time. So, so they're kind of, they're treating the artificial grammar experiment in the same analogous way as they're treating this weird like light bulb experiment. Right, and that's sort of the argument that this is a general phenomenon, this is a general way that children process information and not a a language specific way. Is that, hopefully that, yeah, like, so, now also people can do things like simulations using neural networks and we'll we'll talk, uh. Um, for some reason you're interested in this, there's lots of readings. There's a whole lot of stuff where people have to use. Computational simulations, because if you're cos some of the arguments around um nativism is that Chomsky was like, learning can't do this. And if learning can't do it, then it has to be innate, right? So then one way to counter that is to develop computational models that like can learn language. And you go, well, there's no, there's no, you know, universal grammar in this computer. And it learned language somehow. So the law, so, so that sort of demonstrates that learning is that the learnability is possible. Right? Now, when people were doing this type of experiments, it was in the 80s and 90s, so these would be what, what we call small language models. Um when we had way, you know, we didn't have GPU centres being trained on the entire internet, you would kind of like try to kind of give small amounts of language, see how much the computer models could learn, what small amount of data could they make inferences from, stuff like that. Although the recent technological advances has changed us quite a bit. So let's talk about chat GPT. Um, so chat GPT is an artificial neural network built on the same type of stuff, built using the same basic law, like same different uh mechanisms that a lot of uh cognitive psychologists were using in the eighties to sort of try to understand language learning. Um, so cognitive scientists, cognitive psychologists developed artificial neural networks. The transformer architecture is a particular kind of neural network architecture that was developed about, uh, 10 years ago that has really increased these models' ability to learn language. However, it does, it doesn't mean they're doing it in a human-like way. All right, so, so what is this? On the one, so on the one hand, nativist argument said human syndex cannot be learned from just example sentences. You need some kind of innate constraint on it. And this is GPT 3. So this is way smaller than the new models. GPT 3, so, so look at how many words were like input into GPT 3 to sort of train on, right? 4 to 5 orders of magnitude, what is that, like 10,000 10,000 times the amount of words that a child hears to become fluent in syntax, right? So even if it looks like it's a human speaking. It was trained in a totally different way than a human was trained, right? And if you go from 0 to 5, you can kind of have an estimate of how many words that you have. Heard, right? So, so it's something like they're estimating that, uh, this is log 10, so that humans hear, what is that, like 106, 107 number of words to become fluent in language. Shot TPT Quote heard 1011 number of words to become fluent in language. Right, so even if it's even if it's endpoint looks very much like a human talking to you, it got there in a very different way. Right? The human, we are sort of much more efficient. Another way also I think about how much energy is being used, how much electrical energy is used in your brain. Much less than a data centre, right? So the engineer, the amount of energy use, so we are a much more efficient intelligent system. Even if these AI systems sort of mimic a lot of what humans can do. Right. Now you could imagine, but what's the difference? So, if you're a nativist, you go, that's what the nativism stuff is, right? Our genetic endowment is the stuff that is is allowing us to learn this so much more efficiently. Other people are more like, well, it's really about how we have embodied experiences, right? And the, in the, in the language model has no vision and has no, it can't move around. Some people go, if it wasn't a robot, maybe then it would be much more efficient or something like that. There's a few different situations that you could uh or they're much worse, you know, we're not using all the it's ITPD isn't using all the social kinds of cues that we use. There's other explanations for it. Or you could say it's a completely different computational system. There's a few different sort of possibilities. But that is something to uh uh oh too long on this. If you're interested, so here is an article that goes, see, Chachi Pti shows Chomsky is wrong, and here's a video Chomsky saying this has nothing to do with anything I've ever said. So he's not convinced. If you're, if you're interested, you can watch those. I mean he's still hompsy's still kicking. Anyway, ah. We, you know, we're lucky to still have them. Uh, how can we use AI as part, right? So, but anyway, we'll, we'll watch all those. There was like a rumour like two years ago that he died, but then it was like he's still alive. He is like 98. Um So, all right, so a summary these constructionist approaches. So the constructionist approaches argues that domain general mechanisms such as statistical learning and analogical abstraction and all the stuff that chat GPT does is more powerful than the general developmental mechanisms that Chomsky first argue against and can account, account for language learning. Infants can segment words and form grammatical categories via statistics and distributions. Children do make structural dependence errors when the statistics support such errors, like in the can verbs. Remember, despite UG claims. Children's early syntactic productions are verb-specific. Their early abstraction accounts rejects that specific syntax, however, we then just said how constructionists have encountered this rejection. And the argument is sensitive periods are related to domain general brain maturation, not language module. Changes All right, now we'll quickly get into culture and this should still all end on time. All right. Talking about two parts.